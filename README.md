# üìå Automated Testing of LLM (GPT-4o) API with Python

üéØ Project Overview

This project focuses on automated testing of Large Language Models (LLMs) using OpenAI's GPT-4o-mini. The goal is to ensure that the model provides accurate, grammatically correct, and reliable responses in different scenarios.

üîπ Key Features

‚úÖ Functional Testing ‚Äì Verifies the correctness of factual responses.
‚úÖ Grammar & Rephrasing Tests ‚Äì Ensures correct sentence structure.
‚úÖ Automated Test Execution ‚Äì Uses Pytest for validation.
‚úÖ Prompt Engineering ‚Äì Optimizes responses using different role settings.
‚úÖ Customizable API Calls ‚Äì Adjusts temperature, max_tokens, and other parameters.
‚úÖ BLEU Score Evaluation ‚Äì Compares generated and expected responses.

---

## üöÄ **Installation & Setup**

### **1Ô∏è‚É£ Clone the repository**

```sh
git clone https://github.com/your-repo/llm-testing.git
```

### **2Ô∏è‚É£ Create a virtual environment (recommended)**

```sh
python -m venv venv
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate    # Windows
```

### **3Ô∏è‚É£ Set up OpenAI API Key**

Create a `.env` file or export the key in your terminal:

```sh
export OPENAI_API_KEY="your_api_key"
```

For Windows (PowerShell):

```sh
$env:OPENAI_API_KEY="your_api_key"
```

---

## üìå **Test Functions**

### **1Ô∏è‚É£ Functional Testing: Factual Responses**

This test verifies if GPT-4o provides **correct answers** to factual questions.

```python
@pytest.mark.parametrize("prompt, expected", [
    ("–ö–∞–∫–∞—è —Å—Ç–æ–ª–∏—Ü–∞ –§—Ä–∞–Ω—Ü–∏–∏?", "–ü–∞—Ä–∏–∂"),
    ("–ö—Ç–æ –Ω–∞–ø–∏—Å–∞–ª '–í–æ–π–Ω—É –∏ –º–∏—Ä'?", "–¢–æ–ª—Å—Ç–æ–π"),
    ("–°–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –°–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?", "8"),
])
def test_gpt_responses(prompt, expected):
    response = chat_with_gpt(prompt)
    assert expected in response, f"Expected '{expected}', but got '{response}'"
```

### **2Ô∏è‚É£ Grammar & Sentence Correction**

This test checks if GPT-4o can **rephrase incorrect English sentences**.

```python
@pytest.mark.parametrize("prompt_dev, prompt, expected", [
    ("Your task is to convert the phrase into correct English",
     "capital France?",
     "What is the capital of France?")
])
def test_gpt_correctEnglish(prompt_dev, prompt, expected):
    response = chat_with_gpt2(prompt_dev, prompt)
    assert expected in response, f"Expected '{expected}', but got '{response}'"
```

---

3Ô∏è‚É£ Evaluating GPT-4o Output with BLEU Score

This script test_metrics.py runs a test prompt and evaluates GPT-4o‚Äôs response using the BLEU metric.

---

## üìå **Running the Tests**

To execute all tests, run:

```sh
python3 -m pytest test_functional.py  
```

‚úÖ **Expected Output (Example):**

```
‚úÖ test_gpt_responses[–ö–∞–∫–∞—è —Å—Ç–æ–ª–∏—Ü–∞ –§—Ä–∞–Ω—Ü–∏–∏?] - PASSED
‚úÖ test_gpt_responses[–ö—Ç–æ –Ω–∞–ø–∏—Å–∞–ª '–í–æ–π–Ω—É –∏ –º–∏—Ä'?] - PASSED
‚ùå test_gpt_correctEnglish[capital France?] - FAILED
```

If a test fails, **analyze the response and adjust prompts or model settings.**

---
